---
title: 'MSiA 400: ESD - Homework 3'
author: "Kristiyan Dimitrov"
date: "12/1/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Exercise 1
### Part c)

```{r}
# install.packages("gtools")
library(gtools)
rdirichlet(1,alpha = c(1,1,1,1,1,1)) # Testing out rdirichlet
```

```{r}
sum(rdirichlet(1,alpha = c(1,1,1,1,1,1))) # Checking important quality of dirichlet distribution
```

```{r}
set.seed(57)
n = 1000000
theta = c(19, 12, 10, 26, 19, 20) # This is my "Gibbs Sampler" 
# From the Lab 6 slides, we see that the Posterior is ~ theta^(k_i + alpha_i -1) * (1-theta)^(n - k_i + alpha_i -1) ??
nSamples = rdirichlet(n, alpha = theta)
head(nSamples)
```

```{r}
# Plotting Histograms
bins = seq(0,1,0.01) # Setting bins for histogram

for (i in seq(1,6)){
  hist(nSamples[,i], breaks = bins, main = paste("Histogram for Theta", toString(i))) 
}
```

## Problem 2

```{r}
gradAdmit = read.csv('gradAdmit.csv') #Importing data
str(gradAdmit)
```

```{r}
set.seed(1100) # Setting the random seed to ensure reproducibility of results

n = nrow(gradAdmit) # number of students in dataset

# Sampling approximately 20% of the indices
testIndices = sample.int(n = n, size = floor(.2*n), replace = F) #80 indices, randomly drawn from 1-400

# Taking all the data except that at the sampled indices as training data
trainData = gradAdmit[-testIndices,]
str(trainData)
```

```{r}
# Taking all the data at the sampled indices as training data
testData = gradAdmit[testIndices,]
str(testData)
```

### a)

Class balance for admitted students in testData
```{r}
testBalance = nrow(testData[testData$admit == 1,]) / nrow(testData)
testBalance
```

Class balance for admitted students in trainingData
```{r}
trainBalance = nrow(trainData[trainData$admit == 1,]) / nrow(trainData)
trainBalance
```

### b)

```{r}
library(e1071) # Loading required library with SVM function
# Training best model from part b) on full training data
# Recall, our best model uses: Radial Basis Kernel with gamma = 1.3 and cost = 0.8

svm_best = svm(factor(admit)~., kernel = 'radial', gamma = 1.3, cost = 0.8, data=trainData)
summary(svm_best)
```

```{r}
# Applying best model to our untouched test data
test_pred = predict(svm_best, newdata=testData, type = 'response')
is.factor(test_pred)
```


```{r}
library(caret)
confusionMatrix(test_pred, factor(testData$admit), positive = "1")
```

Note that the precision is not calculated above. It is 2/(2+8) = 20%. Looks like it might be equal to the Pos Pred Value

### Part c) 

```{r}
# Percentage of Oversampling needed to get 50% balance
overSamplingNeeded = .5 * nrow(trainData) / nrow(trainData[trainData$admit == 1,]) 
paste(round(overSamplingNeeded*100,3),"%")
```

```{r}
# install.packages("DMwR")
library(DMwR)
?SMOTE
trainData$admit <- as.factor(trainData$admit)
newTrainData <- SMOTE(admit ~ . , data = trainData, perc.over = overSamplingNeeded*100)
nrow(newTrainData)
```

```{r}
# Calculating new class balance
newTrainDataBalance = nrow(newTrainData[newTrainData$admit == 1,]) / nrow(newTrainData)
newTrainDataBalance
```

### Part d)

```{r}
# Training on newTrainData
svmBalancedData = svm(admit~., kernel = 'radial', gamma = 1.3, cost = 0.8, data = newTrainData)
summary(svmBalancedData)
```

```{r}
# Applying newly trained model on same test data
test_pred = predict(svmBalancedData , newdata=testData, type = 'response')
test_pred
```


```{r}
# Confusion matrix for predictions & testData with model trained on balanced data
confusionMatrix(test_pred, factor(testData$admit), positive = "1")
```

The old model had Sensitivity (same as Recall) = .07, while the new one has .46 Sensitivity. This is a significant improvement
The old model had Specificity = .85, while the new one has .518 Specificity. This is a significant decrease.
The new model has Precision 12/(26+12) = 31.58%, which is a significant improvement over the old model's 20%.

We note that the overall accuracy is lower for the new model at 50%, while it was 60% for the old one.
Accuracy is not everything and could be misleading.

## Exercise 3

### Part a)

The CDF of the exponential distribution is 1 - exp(-x).
Therefore, p( x >= 10\*pi) = 1 - p(x <= 10\*pi) = 1 - CDF(10\*pi) = 1 - (1 - exp(-10\*pi)) = exp(-10\*pi)
```{r}
exp(-10*pi)
```

The above value is our probability of drawing x >= 10pi from the exponential distribution.

```{r}

set.seed(43)
n = 1000000 # number of samples
x = runif(n,0,1) # First I draw random values from uniform distribution [0,1]
y = -log(x) # Then I plug into the inverse of the CDF to draw from the exponential distribution

g <- function(a){ # This is our piece-wise g(x) function.
  if (a > pi*10){
    return (sin(a))
  } else {
    return (0)
  }
}

sum = 0
# Finally, I plug all the values sampled from the exponential distribution into g(x) and estimate the integral by taking the average.
for (i in seq(n)){
  sum = sum + g(y[i])
}
sum/n
```

As expected, our numerical estimation is 0, because all the values we sampled from the exponential distribution are less than 10pi.
As a result, when we evaluate g(x) at those points, we will get 0 i.e. we will fall within the piecewise part of the function that is 0.
The above result is intuitive - the tail of the exponential distribution after 10pi is very skinny.

### Part d)

Let's sample from exp(10pi-x).
The inverse of the CDF is 10pi - log(x)
Below we sample from that

```{r}
set.seed(43)
n = 1000000 # number of samples
x = runif(n,0,1) # First I draw random values from uniform distribution [0,1]
yShifted = 10*pi-log(x)
head(yShifted)
```

Now we plug into g(x). However, we need to multiply by the likelihood ratio p(x)/p_star(x) to compensate for the fact we are drawing from p_star(x) instead of p(x)

```{r}
# Define our p_star(x) function
p_star <- function(a){
  if (a >= pi*10){
    return (exp(10*pi-a))
  } else {
    return (0)
  }
}
```

```{r}
# Now we estimate the integral as the average value of all the x drawn from the shifted exponential 
# and then plugged into g(x) and multiplied by p(x)/p_star(x)
sumShifted = 0

for (i in seq(n)){
  sumShifted = sumShifted + g(yShifted[i])*exp(-yShifted[i])/p_star(yShifted[i])
}
sumShifted/n
```

```{r}
# We check what our error is
abs(sumShifted/n - exp(-10*pi)/2)
```

Our estimate appears to be fairly good.


### Part e) - an alternative biasing function
I want to try and use an alternative biasing function.
I want to use exponential distribution, which is "skinnier" closer to 0, and "fatter" further away from 0.
Ideally, it will be larger than exp(-x) for values >= 10*pi and smaller otherwise i.e. 
I want to find lambda s.t. 

lambda * exp(-lambda*10pi) = e(-10pi)

After some manipulation, that boils down to lambda * ln(lambda) + 1 = 0.
It turns out this has no roots!
A visual inspection of the function graph confirms this
```{r}
curve(x*log(x)+1)
```

After taking derivative of lambda * ln(lambda) + 1 w.r.t. lambda and setting that to 0, I find that the lowest value for the function is achieved at lambda = exp(-1)

```{r}
lambdaBest = exp(-1)
lambdaBest
```

Now, If I sample from biasing exponential distribution with lambdaBest I will also have to multiply by the likelihood ratio.
p(x)/p_star(x) will be  exp(-x) / lambdaBest\*exp(-lambdaBest\*x) = 1/ lambdaBest*exp(lambdaBest)
```{r}
set.seed(43)
n = 1000000 # number of samples
x = runif(n,0,1) # First I draw random values from uniform distribution [0,1]
yNew = -log(x)/lambdaBest # Then I plug into the inverse of the CDF to draw from the exponential distribution
likelihoodRatio = 1 / (lambdaBest*exp(lambdaBest))

sumNew = 0
for (i in seq(n)){
  sumNew = sumNew + g(yNew[i])*likelihoodRatio
}
sumNew/n
```

It appears that our estimate for the integral is very poor by using this biasing function.
But it was worth investigating :)
